{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddatta/anaconda3/envs/AD_v3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "[nltk_data] Downloading package wordnet to /home/ddatta/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ddatta/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.io\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import networkx as nx\n",
    "# import utils.preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as sklearn_stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_label = pd.read_csv('raw/DBLP/author_label.txt', sep='\\t', header=None, names=['author_id', 'label', 'author_name'], keep_default_na=False, encoding='utf-8')\n",
    "paper_author = pd.read_csv('raw/DBLP/paper_author.txt', sep='\\t', header=None, names=['paper_id', 'author_id'], keep_default_na=False, encoding='utf-8')\n",
    "paper_conf = pd.read_csv('raw/DBLP/paper_conf.txt', sep='\\t', header=None, names=['paper_id', 'conf_id'], keep_default_na=False, encoding='utf-8')\n",
    "paper_term = pd.read_csv('raw/DBLP/paper_term.txt', sep='\\t', header=None, names=['paper_id', 'term_id'], keep_default_na=False, encoding='utf-8')\n",
    "papers = pd.read_csv('raw/DBLP/paper.txt', sep='\\t', header=None, names=['paper_id', 'paper_title'], keep_default_na=False, encoding='cp1252')\n",
    "terms = pd.read_csv('raw/DBLP/term.txt', sep='\\t', header=None, names=['term_id', 'term'], keep_default_na=False, encoding='utf-8')\n",
    "confs = pd.read_csv('raw/DBLP/conf.txt', sep='\\t', header=None, names=['conf_id', 'conf'], keep_default_na=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = author_label['author_id'].to_list()\n",
    "paper_author = paper_author[paper_author['author_id'].isin(authors)].reset_index(drop=True)\n",
    "valid_papers = paper_author['paper_id'].unique()\n",
    "print('Number of papers :', len(valid_papers))\n",
    "\n",
    "papers = papers[papers['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "paper_conf = paper_conf[paper_conf['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "print('Number of papers :', len(paper_conf))\n",
    "\n",
    "paper_term = paper_term[paper_term['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "valid_terms = paper_term['term_id'].unique()\n",
    "terms = terms[terms['term_id'].isin(valid_terms)].reset_index(drop=True)\n",
    "\n",
    "# term lemmatization and grouping\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma_id_mapping = {}\n",
    "lemma_list = []\n",
    "lemma_id_list = []\n",
    "i = 0\n",
    "for _, row in terms.iterrows():\n",
    "    i += 1\n",
    "    lemma = lemmatizer.lemmatize(row['term'])\n",
    "    lemma_list.append(lemma)\n",
    "    if lemma not in lemma_id_mapping:\n",
    "        lemma_id_mapping[lemma] = row['term_id']\n",
    "    lemma_id_list.append(lemma_id_mapping[lemma])\n",
    "terms['lemma'] = lemma_list\n",
    "terms['lemma_id'] = lemma_id_list\n",
    "\n",
    "term_lemma_mapping = {row['term_id']: row['lemma_id'] for _, row in terms.iterrows()}\n",
    "lemma_id_list = []\n",
    "for _, row in paper_term.iterrows():\n",
    "    lemma_id_list.append(term_lemma_mapping[row['term_id']])\n",
    "paper_term['lemma_id'] = lemma_id_list\n",
    "\n",
    "paper_term = paper_term[['paper_id', 'lemma_id']]\n",
    "paper_term.columns = ['paper_id', 'term_id']\n",
    "paper_term = paper_term.drop_duplicates()\n",
    "terms = terms[['lemma_id', 'lemma']]\n",
    "terms.columns = ['term_id', 'term']\n",
    "terms = terms.drop_duplicates()\n",
    "\n",
    "# filter out stopwords from terms\n",
    "stopwords = sklearn_stopwords.union(set(nltk_stopwords.words('english')))\n",
    "stopword_id_list = terms[terms['term'].isin(stopwords)]['term_id'].to_list()\n",
    "paper_term = paper_term[~(paper_term['term_id'].isin(stopword_id_list))].reset_index(drop=True)\n",
    "terms = terms[~(terms['term'].isin(stopwords))].reset_index(drop=True)\n",
    "len(terms)\n",
    "\n",
    "author_label = author_label.sort_values('author_id').reset_index(drop=True)\n",
    "papers = papers.sort_values('paper_id').reset_index(drop=True)\n",
    "terms = terms.sort_values('term_id').reset_index(drop=True)\n",
    "confs = confs.sort_values('conf_id').reset_index(drop=True)\n",
    "\n",
    "print('Number of conferences ', len(confs))\n",
    "print('Number of authors ', len(author_label))\n",
    "print('Number of terms ', len(terms))\n",
    "print('Number of papers ', len(papers))\n",
    "\n",
    "authors_list = list(author_label['author_id'])\n",
    "papers_list = list(papers['paper_id'])\n",
    "term_list = list(terms['term_id'])\n",
    "conf_list = list(confs['conf_id'])\n",
    "dim = len(authors_list) + len(papers_list) + len(term_list) + len(confs)\n",
    "print(' Total entities :: ', dim)\n",
    "\n",
    "\n",
    "author_id_mapping = {row['author_id']: i for i, row in author_label.iterrows()}\n",
    "paper_id_mapping = {row['paper_id']: i + len(author_label) for i, row in papers.iterrows()}\n",
    "term_id_mapping = {row['term_id']: i + len(author_label) + len(papers) for i, row in terms.iterrows()}\n",
    "conf_id_mapping = {row['conf_id']: i + len(author_label) + len(papers) + len(terms) for i, row in confs.iterrows()}\n",
    "\n",
    "\n",
    "entity_id_map = pd.DataFrame(\n",
    "    columns=['domain', 'entity_id','serial_id']\n",
    ")\n",
    "type_dict = { 'author': author_id_mapping, 'paper': paper_id_mapping, 'term': term_id_mapping, 'conf': conf_id_mapping }\n",
    "for _type,_dict in type_dict.items():\n",
    "    i = list(_dict.keys())\n",
    "    j = list(_dict.values())\n",
    "    _df = pd.DataFrame( data = {'entity_id': i ,'serial_id': j } )\n",
    "    _df['domain'] = _type\n",
    "    entity_id_map = entity_id_map.append(_df, ignore_index=True)\n",
    "\n",
    "    \n",
    "# ======================================================\n",
    "# Save data\n",
    "# ======================================================\n",
    "data_save_path = 'processed_data/DBLP'\n",
    "if not os.path.exists('processed_data'):\n",
    "    os.mkdir('processed_data')\n",
    "if not os.path.exists(data_save_path):\n",
    "    os.mkdir(data_save_path)\n",
    "entity_id_map.to_csv( os.path.join( data_save_path, 'entity_id_mapping.csv') ) \n",
    "\n",
    "# Create graph data\n",
    "nodes_author_df = pd.DataFrame( data = { 'author' : list(author_id_mapping.values()) })\n",
    "nodes_paper_df = pd.DataFrame(  data = { 'paper' : list(paper_id_mapping.values()) } )\n",
    "nodes_term_df = pd.DataFrame( data = { 'term' : list(term_id_mapping.values()) } )\n",
    "nodes_conf_df = pd.DataFrame(  data = { 'conf' : list(conf_id_mapping.values()) } )\n",
    "\n",
    "nodes_author_df.to_csv(os.path.join(data_save_path,'nodes_author.csv'),index = False)\n",
    "nodes_paper_df.to_csv(os.path.join(data_save_path,'nodes_paper.csv'),index = False)\n",
    "nodes_term_df.to_csv(os.path.join(data_save_path,'nodes_term.csv'),index = False)\n",
    "nodes_conf_df.to_csv(os.path.join(data_save_path,'nodes_conf.csv'),index = False)\n",
    "\n",
    "PA_edge_list = []\n",
    "for _, row in paper_author.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = author_id_mapping[row['author_id']]\n",
    "    PA_edge_list.append((idx1,idx2))\n",
    "    \n",
    "df = pd.DataFrame ( data =  np.array(PA_edge_list), columns = ['source','target'])\n",
    "fpath = os.path.join(data_save_path, 'PA_edges.csv')\n",
    "df.to_csv(fpath, index=False)\n",
    "    \n",
    "PT_edge_list = []\n",
    "for _, row in paper_term.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = term_id_mapping[row['term_id']]\n",
    "    PT_edge_list.append((idx1,idx2))\n",
    "\n",
    "df = pd.DataFrame ( data =  np.array(PT_edge_list), columns = ['source','target'])\n",
    "fpath = os.path.join(data_save_path, 'PT_edges.csv')\n",
    "df.to_csv(fpath, index=False)\n",
    "    \n",
    "\n",
    "PC_edge_list = []\n",
    "for _, row in paper_conf.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = conf_id_mapping[row['conf_id']]\n",
    "    PC_edge_list.append((idx1,idx2))\n",
    "\n",
    "df = pd.DataFrame ( data = np.array(PC_edge_list), columns = ['source','target'])\n",
    "fpath = os.path.join(data_save_path, 'PC_edges.csv')\n",
    "df.to_csv(fpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Create data for HIN2Vec\n",
    "# ==============================\n",
    "\n",
    "df = pd.DataFrame(columns=['node1', 'node2','rel'])\n",
    "for edge in PA_edge_list:\n",
    "    df = df.append({'node1':edge[0],'node2':edge[1],'rel': 0},ignore_index=True )\n",
    "\n",
    "for edge in PT_edge_list:\n",
    "    df = df.append({'node1':edge[0],'node2':edge[1],'rel': 1},ignore_index=True )\n",
    "    \n",
    "for edge in PC_edge_list:\n",
    "    df = df.append({'node1':edge[0],'node2':edge[1],'rel': 2},ignore_index=True )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['node1'] = df['node1'].astype(int)\n",
    "df['node2'] = df['node2'].astype(int)\n",
    "df['rel'] = df['rel'].astype(int)\n",
    "fpath = os.path.join(data_save_path,'hin2vec_dblp_input.txt')\n",
    "df.to_csv( fpath, index = None, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node1</th>\n",
       "      <th>node2</th>\n",
       "      <th>rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4057</td>\n",
       "      <td>262</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4058</td>\n",
       "      <td>263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4059</td>\n",
       "      <td>263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4059</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4060</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119778</th>\n",
       "      <td>18380</td>\n",
       "      <td>26127</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119779</th>\n",
       "      <td>18381</td>\n",
       "      <td>26127</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119780</th>\n",
       "      <td>18382</td>\n",
       "      <td>26127</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119781</th>\n",
       "      <td>18383</td>\n",
       "      <td>26127</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119782</th>\n",
       "      <td>18384</td>\n",
       "      <td>26127</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119783 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        node1  node2  rel\n",
       "0        4057    262    0\n",
       "1        4058    263    0\n",
       "2        4059    263    0\n",
       "3        4059    264    0\n",
       "4        4060    266    0\n",
       "...       ...    ...  ...\n",
       "119778  18380  26127    2\n",
       "119779  18381  26127    2\n",
       "119780  18382  26127    2\n",
       "119781  18383  26127    2\n",
       "119782  18384  26127    2\n",
       "\n",
       "[119783 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
